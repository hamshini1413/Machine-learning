import pandas as pd
import numpy as np
import matplotlib.pyplot as plt  
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.model_selection import train_test_split


df = pd.read_csv(r"C:\Users\nanga\OneDrive\Desktop\mp.csv")
print("First 5 rows:")
print(df.head())
df.head
df = df[['horsepower', 'mpg']]
print(df)
df = df.dropna()
print("\nAfter Cleaning:")
print(df.info())


df['horsepower'] = pd.to_numeric(df['horsepower'], errors='coerce')

plt.scatter(df['horsepower'], df['mpg'])
plt.xlabel("Horsepower")
plt.ylabel("MPG")
plt.title("Horsepower vs MPG")
plt.show()
X = df[['horsepower']]
y = df['mpg']

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42)

model = LinearRegression()
model.fit(X_train, y_train)

y_pred_sklearn = model.predict(X_test)

print("\n--- Scikit Learn Results ---")
print("Slope:", model.coef_[0])
print("Intercept:", model.intercept_)
print("MSE:", mean_squared_error(y_test, y_pred_sklearn))
print("R2 Score:", r2_score(y_test, y_pred_sklearn))

X_train_np = np.c_[np.ones(len(X_train)), X_train.values]
y_train_np = y_train.values.reshape(-1, 1)

theta = np.linalg.inv(X_train_np.T @ X_train_np) @ X_train_np.T @ y_train_np

print("\n--- Normal Equation Results ---")
print("Intercept:", theta[0][0])
print("Slope:", theta[1][0])
X_gd = X_train.values.flatten()
y_gd = y_train.values

m = len(X_gd)
theta0 = 0
theta1 = 0
learning_rate = 0.0001
epochs = 1000
for i in range(epochs):
    y_pred = theta0 + theta1 * X_gd
    
    d_theta0 = (-2/m) * np.sum(y_gd - y_pred)
    d_theta1 = (-2/m) * np.sum(X_gd * (y_gd - y_pred))
    
    theta0 -= learning_rate * d_theta0
    theta1 -= learning_rate * d_theta1

print("\n--- Gradient Descent Results ---")
print("Intercept:", theta0)
print("Slope:", theta1)
plt.scatter(X_test, y_test, label="Actual")
plt.plot(X_test, y_pred_sklearn, label="Sklearn Prediction")
plt.xlabel("Horsepower")
plt.ylabel("MPG")
plt.legend()
plt.show()
